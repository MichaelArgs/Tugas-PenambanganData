{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MichaelArgs/Tugas-PenambanganData/blob/main/Klasifikasi_Berita.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f3b6880",
      "metadata": {
        "id": "4f3b6880"
      },
      "source": [
        "## 1. Persiapan dan Import Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "845f32aa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "845f32aa",
        "outputId": "12538073-4f9a-404d-cf8f-98c880009261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\n",
        "# Bagian 1: Import Library yang Diperlukan\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from nltk.stem import PorterStemmer\n",
        "import re\n",
        "\n",
        "# Download NLTK resources (untuk digunakan di Google Colab)\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd0a47dd",
      "metadata": {
        "id": "bd0a47dd"
      },
      "source": [
        "## 2. Preprocessing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "827f1a02",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "827f1a02",
        "outputId": "2e02199b-dd0c-43e0-f060-3549001a8b67"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'dataset.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-200a9f8076ff>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Bagian 2: Preprocessing Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Memuat dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset.csv'"
          ]
        }
      ],
      "source": [
        "\n",
        "# Bagian 2: Preprocessing Data\n",
        "# Memuat dataset\n",
        "data = pd.read_csv('dataset.csv')\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"\n",
        "    Membersihkan teks dari angka, tanda baca, dan spasi berlebih\n",
        "    \"\"\"\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.lower()\n",
        "    return text\n",
        "\n",
        "# Terapkan pembersihan\n",
        "data['Content'] = data['Content'].apply(clean_text)\n",
        "\n",
        "# Tokenisasi\n",
        "data['Tokens'] = data['Content'].apply(word_tokenize)\n",
        "\n",
        "# Stopword Removal\n",
        "stop_words = set(stopwords.words('indonesian'))\n",
        "data['Filtered'] = data['Tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "data['Stemmed'] = data['Filtered'].apply(lambda x: [stemmer.stem(word) for word in x])\n",
        "\n",
        "# Normalisasi\n",
        "data['Processed_Text'] = data['Stemmed'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "# Menampilkan hasil preprocessing\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "24146074",
      "metadata": {
        "id": "24146074"
      },
      "source": [
        "## 3. TF-IDF Vectorization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d5f7079",
      "metadata": {
        "id": "7d5f7079"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Bagian 3: TF-IDF Vectorization\n",
        "# Vectorisasi teks menggunakan TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=10000)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data['Processed_Text'])\n",
        "\n",
        "# Menampilkan TF-IDF terms dan bobotnya\n",
        "tfidf_words = tfidf_vectorizer.get_feature_names_out()\n",
        "tfidf_weights = tfidf_matrix.toarray()\n",
        "tfidf_df = pd.DataFrame(tfidf_weights, columns=tfidf_words)\n",
        "tfidf_df.head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bce03ef",
      "metadata": {
        "id": "8bce03ef"
      },
      "source": [
        "## 4. Implementasi Algoritma\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f25e9a98",
      "metadata": {
        "id": "f25e9a98"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Bagian 4: Implementasi VSM (Vector Space Model)\n",
        "# Encode label kategori\n",
        "label_encoder = LabelEncoder()\n",
        "data['Label'] = label_encoder.fit_transform(data['Category'])\n",
        "\n",
        "# Split dataset menjadi train dan test\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, data['Label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Membaca dokumen uji\n",
        "with open('Test2.txt', 'r') as file:\n",
        "    test_doc = file.read()\n",
        "\n",
        "# Preprocessing dokumen uji\n",
        "cleaned_test_doc = clean_text(test_doc)\n",
        "tokenized_test_doc = word_tokenize(cleaned_test_doc)\n",
        "filtered_test_doc = [word for word in tokenized_test_doc if word not in stop_words]\n",
        "stemmed_test_doc = [stemmer.stem(word) for word in filtered_test_doc]\n",
        "normalized_test_doc = ' '.join(stemmed_test_doc)\n",
        "\n",
        "# Vectorisasi dokumen uji menggunakan TF-IDF\n",
        "test_vector = tfidf_vectorizer.transform([normalized_test_doc])\n",
        "\n",
        "# Hitung kemiripan kosinus\n",
        "cos_similarities = cosine_similarity(test_vector, X_train)\n",
        "predicted_index = np.argmax(cos_similarities)\n",
        "\n",
        "# Menggunakan array agar tidak terjadi KeyError\n",
        "y_train_array = np.array(y_train)\n",
        "predicted_label = y_train_array[predicted_index]\n",
        "\n",
        "# Mengonversi label yang diprediksi ke kategori asli\n",
        "predicted_category = label_encoder.inverse_transform([predicted_label])[0]\n",
        "print(f\"Prediksi Kategori untuk Dokumen Uji: {predicted_category}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "415fc8d5",
      "metadata": {
        "id": "415fc8d5"
      },
      "source": [
        "## 5. Evaluasi Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "198f57db",
      "metadata": {
        "id": "198f57db"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Bagian 5: Evaluasi Model\n",
        "# Hitung kemiripan kosinus antara data uji dan data latih\n",
        "cos_similarities = cosine_similarity(X_test, X_train)\n",
        "\n",
        "# Prediksi label berdasarkan kemiripan tertinggi\n",
        "predicted_indices = cos_similarities.argmax(axis=1)\n",
        "\n",
        "# Menggunakan array agar tidak terjadi KeyError\n",
        "y_train_array = np.array(y_train)\n",
        "y_pred = y_train_array[predicted_indices]\n",
        "\n",
        "# Konversi hasil prediksi ke kategori yang sebenarnya\n",
        "y_pred_labels = label_encoder.inverse_transform(y_pred)\n",
        "y_true_labels = label_encoder.inverse_transform(y_test)\n",
        "\n",
        "# Tampilkan classification report\n",
        "report = classification_report(y_true_labels, y_pred_labels, target_names=label_encoder.classes_)\n",
        "print(\"Classification Report:\")\n",
        "print(report)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}